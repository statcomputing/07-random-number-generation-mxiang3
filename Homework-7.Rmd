---
title: "Homework 7"
author: "Meiruo Xiang"
date: "10/22/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MCMCpack)
```

# 5.3.1 Rejection sampling

### 1. Find the value of the normalizing constant for g.

$$
\begin{aligned}
\int_0^\infty x^{\theta-1}e^{-x}dx & \propto Gamma(\theta,1)\\
\int_0^\infty x^{\theta-0.5}e^{-x}dx & \propto Gamma(\theta+0.5,1)
\end{aligned}
$$

Then,

$$
\begin{aligned}
& C \int_0^\infty(2x^{\theta-1}+x^{\theta-0.5})e^{-x}dx\\
& = C*(2\int_0^\infty x^{\theta-1}e^{-x}dx+ \int_0^\infty x^{\theta-0.5}e^{-x}dx) \\
& = C*[2\Gamma(\theta)+\Gamma(\theta+0.5)] \\
& = 1
\end{aligned}
$$

Therefore,
$$C=\frac{1}{2\Gamma(\theta)+\Gamma(\theta+0.5)}$$

g is a mixture of $Gamma(\theta,1)$ and $Gamma(\theta +0.5,1)$ distributions.

$$g(x)=\frac{2}{2\Gamma(\theta)+\Gamma(\theta+0.5)} Gamma(\theta,1) + \frac{1}{2\Gamma(\theta)+\Gamma(\theta+0.5)} Gamma(\theta +0.5,1)$$

### 2. Design a procedure (pseudo-code) to sample from g.

```{r}
n <- 10000
theta <- 6
C <- 1 / (2 * gamma(theta) + gamma(theta + 0.5))

# generate random samples from g
rg <- function(n, theta){
  u <- runif(n)
  rtheta <- ifelse(u < 2 / 3, theta, theta + 0.5)
  rgamma(n, shape = rtheta, scale = 1)
}
x <- rg(n, theta)
# simulated density
fx <- 2 * dgamma(x, theta) + dgamma(x, theta + 0.5)
# true density
truex <- (2 * x^(theta - 1) + x^(theta - 0.5)) * exp(-x)
# compare two densities
plot(x, fx, col = 2, ylab = "density", 
      main = "kernel density estimation of g") 
par(new = TRUE)                             
plot(x, truex, col = 3, axes = FALSE, ylab = "")
legend("topright", legend=c("Simulated", "True"),
       col=c(2, 3), lty=1, cex=1)
```

### 3. Design a procedure (pseudo-code) to use rejection sampling to sample from f using g as the instrumental distribution.

```{r}
f <- function(x, theta){
  sqrt(4+x) * x^(theta - 1) * exp(-x)
} 
g <- function(x, theta){
  (2 * x^(theta - 1) + x^(theta - 0.5)) * exp(-x)
}
x.rej <- rep(NA, n)
f.true <- rep(NA, n)
for (i in 1:n){
  while (TRUE){
    cand <- rg(1, theta)
    ratio <- f(cand, theta) / g(cand, theta)
    u <- runif(1)
    if (u< ratio) break
  }
  x.rej[i] <- cand              # rejection sampling
  f.true[i] <- f(cand, theta)   # true density of f
}

# compare two densities
f.rej <- density(x.rej)
plot(density(x.rej), col = 2, xlab = "x", ylab = "density", 
      main = "kernel density estimation of f")              
par(new = TRUE)                             
plot(x.rej, f.true, col = 3, axes = FALSE, xlab = "", ylab = "")
legend("topright", legend=c("Rejection Sampling", "True"),
       col=c(2, 3), lty=1, cex=1)
```


# 6.3.1  Normal mixture revisited

Priors are: $\mu_1, \mu_2 \sim Normal(0, 10^2)$, $1/\sigma_1^2, 1/\sigma_2^2 \sim Gamma(0.5, 10)$ then $\sigma_1^2, \sigma_2^2 \sim InvGamma(0.5, 1/10)$, mixing rate $\delta \sim Uniform(0,1)$.

Since all the priors are independent, the likelihood is 

  $$f(x|\mu_1,\mu_2,\sigma^2_1,\sigma^2_2,\delta) \propto \delta *N(\mu_1,\sigma^2_1) + (1-\delta)*N(\mu_2,\sigma^2_2) $$

Therefore, the posterior is 

$$
\begin{aligned}
& f(\mu_1,\mu_2,\sigma^2_1,\sigma^2_2,\delta|x) \\
& \varpropto  \pi(\mu_1)  \pi(\mu_2)  \pi(\sigma^2_1)  \pi(\sigma^2_2) \pi(\delta) * f(x|\mu_1,\mu_2,\sigma^2_1,\sigma^2_2,\delta)\\
& \varpropto  \pi(\mu_1)  \pi(\mu_2)  \pi(\sigma^2_1)  \pi(\sigma^2_2) \pi(\delta) *[\delta *\frac{1}{\sigma_1}exp\{-\frac{(x -\mu_1)^2}{2\sigma^2_1}\} + (1-\delta) *\frac{1}{\sigma_2}exp\{-\frac{(x -\mu_2)^2}{2\sigma^2_2}\}] 
\end{aligned}
$$

### 1. generate normal mixture data.
```{r}
set.seed(111)
n <- 100
delta <- 0.7
u <- rbinom(n, prob = delta, size = 1)
x <- rnorm(n, ifelse(u == 1, 7, 10), 0.5)
```

### 2. compute the posterior density up to an unknown normalizing constant
```{r}
## theta: [mu1, mu2, var1, var2, delta]
logpost <- function(theta, x) {
  mu1   <- theta[1]
  mu2   <- theta[2]
  var1  <- theta[3]
  var2  <- theta[4]
  delta <- theta[5]
  return(sum(log(delta * dnorm(x, mu1, sqrt(var1)) 
          + (1 - delta) * dnorm(x, mu2, sqrt(var2))))
  + log(dnorm(mu1, 0, 10)) + log(dnorm(mu2, 0, 10))  
  + log(dinvgamma(var1, 0.5, 0.1)) + log(dinvgamma(var2, 0.5, 0.1)))
}
```

### 3. an MCMC based the Gibbs sampler uses the ARMS algorithm
```{r}
mymcmc <- function(niter, thetaInit, x, nburn) {
  p <- length(thetaInit)
  thetaCurrent <- thetaInit
  ## define a function for full conditional sampling  
  logFC <- function(th, idx) {
    theta <- thetaCurrent
    theta[idx] <- th
    logpost(theta, x)
  }
  out <- matrix(thetaInit, niter, p, byrow = TRUE)
  ## Gibbs sampling
  for (i in 2:niter) {
    for (j in 1:p) {
      ## general-purpose arms algorithm
      out[i, j] <- thetaCurrent[j] <- 
        if (j < 3) {                       # mu
          HI::arms(thetaCurrent[j], logFC,
                   function(x, idx) ((x > -50) * (x < 50)), 
                   1, idx = j)
        } 
        else if (j < 5) {                 # sigma^2
          HI::arms(thetaCurrent[j], logFC,
                   function(x, idx) ((x > 0) * (x < 50)), 
                   1, idx = j)
        } 
        else {                             # delta
          HI::arms(thetaCurrent[j], logFC,
                   function(x, idx) ((x > 0) * (x < 1)), 
                   1, idx = j)
        }
    }
  }
  out[-(1:nburn), ]
}

```

### 4. estimation of parameters
```{r}
niter <- 5000
nburn <-1000
thetaInit <- c(0, 0, 2, 2, 0.5)
sim <- mymcmc(niter, thetaInit, x, nburn)

par(mfrow=c(2,2))
plot(ts(sim[,1]), ylab = 'mu1', main = 'mu1')
hist(ts(sim[,1]), xlab = 'mu1', main = '')

plot(ts(sim[,2]), ylab = 'mu2', main = 'mu2')
hist(ts(sim[,2]), xlab = 'mu2', main = '')

plot(ts(sim[,3]), ylab = 'sigmasq1', main = 'sigmasq1')
hist(ts(sim[,3]), xlab = 'sigmasq1', main = '')

plot(ts(sim[,4]), ylab = 'sigmasq2', main = 'sigmasq2')
hist(ts(sim[,4]), xlab = 'sigmasq2', main = '')

plot(ts(sim[,5]), ylab = 'Delta', main = 'Delta')
hist(ts(sim[,5]), xlab = 'Delta', main = '')
```




